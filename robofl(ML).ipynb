{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriramShravya/traffic_prediction/blob/main/robofl(ML).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5ZHt2_I_nGJ"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics roboflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow #from roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"7vYkKz547Z8GPgQrTSMy\")\n",
        "project = rf.workspace(\"traffic-zi58y\").project(\"my-first-project-dcbu4\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "id": "k8TcYV4SAY-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training yolov8 on roboflow dataset\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8s.pt\")  # Better choice for small dataset\n",
        "\n",
        "model.train(\n",
        "    data=\"/content/My-First-Project-1/data.yaml\",\n",
        "    epochs=150,     # or 200 if needed\n",
        "    imgsz=640,\n",
        "    patience=15     # Stop early if no improvement\n",
        ")\n"
      ],
      "metadata": {
        "id": "j0-OQhxJA8tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/My-First-Project-1/data.yaml\n"
      ],
      "metadata": {
        "id": "K3-apMXzNdwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.val()\n"
      ],
      "metadata": {
        "id": "PnH-wgAQN5SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Load trained YOLOv8 model\n",
        "model = YOLO(\"/content/runs/detect/train/weights/best.pt\")\n",
        "\n",
        "# Load input video\n",
        "input_path = \"/content/video10s.mp4\"\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "# Get video properties\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Define output video writer\n",
        "output_path = \"/content/output.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Frame-by-frame inference\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Predict with model\n",
        "    results = model.predict(source=frame, conf=0.5, imgsz=640, verbose=False)\n",
        "\n",
        "    # Draw results on frame\n",
        "    annotated_frame = results[0].plot()  # Draw boxes and labels\n",
        "\n",
        "    # Write annotated frame to output video\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "# Release everything\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"âœ… Output video saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "CFbdIDidOJQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python pandas numpy torchvision\n"
      ],
      "metadata": {
        "id": "6V6jnz1uKNGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n"
      ],
      "metadata": {
        "id": "9QKuVlzQapmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "# Load models\n",
        "auto_model = YOLO(\"/content/runs/detect/train/weights/best.pt\")  # Auto-rickshaw model\n",
        "base_model = YOLO(\"yolov8s.pt\")  # General detection model\n",
        "\n",
        "# Input video\n",
        "video_path = \"/content/video10s.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Output video setup (AVI)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(\"output2_annotated.mp4\", fourcc, fps, (width, height))\n",
        "\n",
        "frame_idx = 0\n",
        "output_data = []\n",
        "\n",
        "# Tracking sets\n",
        "counted_person_ids = set()\n",
        "counted_car_ids = set()\n",
        "counted_bike_ids = set()\n",
        "counted_auto_ids = set()\n",
        "\n",
        "# For speed calculation\n",
        "prev_positions = {}  # track_id: (x_center, y_center)\n",
        "distance_scale = 0.05  # rough scale to simulate speed estimate from pixel/frame\n",
        "\n",
        "# Class IDs\n",
        "PERSON_ID = 0\n",
        "CAR_ID = 2\n",
        "MOTORCYCLE_ID = 3\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run both models\n",
        "    auto_result = auto_model.track(source=frame, conf=0.3, imgsz=640, persist=True, verbose=False)[0]\n",
        "    base_result = base_model.track(source=frame, conf=0.3, imgsz=640, persist=True, verbose=False)[0]\n",
        "\n",
        "    auto_boxes = auto_result.boxes\n",
        "    base_boxes = base_result.boxes\n",
        "\n",
        "    person_boxes = []\n",
        "    vehicle_boxes = []\n",
        "\n",
        "    # Temporary ID lists for this frame\n",
        "    frame_person_ids = set()\n",
        "    frame_car_ids = set()\n",
        "    frame_bike_ids = set()\n",
        "    frame_auto_ids = set()\n",
        "\n",
        "    # Speed tracking\n",
        "    vehicle_speeds = []\n",
        "\n",
        "    # --- Auto-rickshaws ---\n",
        "    auto_tensor = torch.stack([box for box in auto_boxes.xyxy.cpu()]) if auto_boxes else torch.empty((0, 4))\n",
        "    auto_ids = auto_boxes.id.cpu() if auto_boxes and auto_boxes.id is not None else torch.full((0,), -1)\n",
        "\n",
        "    for i, box in enumerate(auto_tensor):\n",
        "        x1, y1, x2, y2 = map(int, box.tolist())\n",
        "        track_id = int(auto_ids[i]) if i < len(auto_ids) else -1\n",
        "        frame_auto_ids.add(track_id)\n",
        "        if track_id not in counted_auto_ids:\n",
        "            counted_auto_ids.add(track_id)\n",
        "        vehicle_boxes.append(box.unsqueeze(0))\n",
        "\n",
        "        cx = (x1 + x2) / 2\n",
        "        cy = (y1 + y2) / 2\n",
        "        if track_id in prev_positions:\n",
        "            dx = cx - prev_positions[track_id][0]\n",
        "            dy = cy - prev_positions[track_id][1]\n",
        "            speed = (dx**2 + dy**2)**0.5 * distance_scale * fps\n",
        "            vehicle_speeds.append(speed)\n",
        "        prev_positions[track_id] = (cx, cy)\n",
        "\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
        "        cv2.putText(frame, f\"Auto ID:{track_id}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
        "\n",
        "    # --- General detections ---\n",
        "    if base_boxes is not None:\n",
        "        base_cls = base_boxes.cls.cpu()\n",
        "        base_xyxy = base_boxes.xyxy.cpu()\n",
        "        base_ids = base_boxes.id.cpu() if base_boxes.id is not None else torch.full((len(base_cls),), -1)\n",
        "\n",
        "        for i, c in enumerate(base_cls):\n",
        "            box = base_xyxy[i].unsqueeze(0)\n",
        "            x1, y1, x2, y2 = map(int, box.squeeze(0).tolist())\n",
        "            track_id = int(base_ids[i])\n",
        "\n",
        "            # Skip overlaps with autos\n",
        "            if auto_tensor.shape[0] > 0 and (box_iou(box, auto_tensor) > 0.5).any():\n",
        "                continue\n",
        "\n",
        "            label = \"\"\n",
        "            is_vehicle = False\n",
        "\n",
        "            if int(c.item()) == PERSON_ID:\n",
        "                person_boxes.append((box, track_id))\n",
        "                continue\n",
        "            elif int(c.item()) == CAR_ID:\n",
        "                label = \"Car\"\n",
        "                frame_car_ids.add(track_id)\n",
        "                if track_id not in counted_car_ids:\n",
        "                    counted_car_ids.add(track_id)\n",
        "                is_vehicle = True\n",
        "            elif int(c.item()) == MOTORCYCLE_ID:\n",
        "                if person_boxes:\n",
        "                    person_tensor = torch.cat([b for b, _ in person_boxes], dim=0)\n",
        "                    if (box_iou(box, person_tensor) > 0.3).any():\n",
        "                        continue\n",
        "                label = \"Motorcycle\"\n",
        "                frame_bike_ids.add(track_id)\n",
        "                if track_id not in counted_bike_ids:\n",
        "                    counted_bike_ids.add(track_id)\n",
        "                is_vehicle = True\n",
        "\n",
        "            if label:\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, f\"{label} ID:{track_id}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "            if is_vehicle:\n",
        "                vehicle_boxes.append(box)\n",
        "\n",
        "                cx = (x1 + x2) / 2\n",
        "                cy = (y1 + y2) / 2\n",
        "                if track_id in prev_positions:\n",
        "                    dx = cx - prev_positions[track_id][0]\n",
        "                    dy = cy - prev_positions[track_id][1]\n",
        "                    speed = (dx**2 + dy**2)**0.5 * distance_scale * fps\n",
        "                    vehicle_speeds.append(speed)\n",
        "                prev_positions[track_id] = (cx, cy)\n",
        "\n",
        "    # --- Walking persons only ---\n",
        "    if person_boxes:\n",
        "        person_tensor = torch.cat([b for b, _ in person_boxes], dim=0)\n",
        "        person_ids = [pid for _, pid in person_boxes]\n",
        "        vehicle_tensor = torch.cat(vehicle_boxes, dim=0) if vehicle_boxes else torch.empty((0, 4))\n",
        "\n",
        "        for i in range(person_tensor.shape[0]):\n",
        "            person_box = person_tensor[i].unsqueeze(0)\n",
        "            track_id = person_ids[i]\n",
        "\n",
        "            is_on_vehicle = False\n",
        "            if vehicle_tensor.shape[0] > 0:\n",
        "                ious = box_iou(person_box, vehicle_tensor)\n",
        "                if ious.max().item() > 0.3:\n",
        "                    is_on_vehicle = True\n",
        "                else:\n",
        "                    px1, py1, px2, py2 = person_box.squeeze().tolist()\n",
        "                    for vbox in vehicle_tensor:\n",
        "                        vx1, vy1, vx2, vy2 = vbox.tolist()\n",
        "                        if px1 > vx1 and px2 < vx2 and py2 < vy2 and py1 > vy1:\n",
        "                            is_on_vehicle = True\n",
        "                            break\n",
        "\n",
        "            if is_on_vehicle:\n",
        "                continue\n",
        "\n",
        "            frame_person_ids.add(track_id)\n",
        "            if track_id not in counted_person_ids:\n",
        "                counted_person_ids.add(track_id)\n",
        "\n",
        "            x1, y1, x2, y2 = map(int, person_box.squeeze().tolist())\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "            cv2.putText(frame, f\"Person ID:{track_id}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "    # Overlay live stats\n",
        "    stats = (\n",
        "        f\"Persons: {len(frame_person_ids)}  Cars: {len(frame_car_ids)}  \"\n",
        "        f\"Bikes: {len(frame_bike_ids)}  Autos: {len(frame_auto_ids)}\"\n",
        "    )\n",
        "    cv2.putText(frame, stats, (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "    avg_speed = np.mean(vehicle_speeds) if vehicle_speeds else 0\n",
        "    vehicle_count = len(frame_car_ids) + len(frame_bike_ids) + len(frame_auto_ids)\n",
        "\n",
        "    # Determine traffic condition\n",
        "    if vehicle_count < 5:\n",
        "        traffic_condition = \"Low\"\n",
        "    elif vehicle_count < 15:\n",
        "        traffic_condition = \"Moderate\"\n",
        "    else:\n",
        "        traffic_condition = \"High\"\n",
        "\n",
        "    # Frame-wise data\n",
        "    output_data.append({\n",
        "        \"frame\": frame_idx,\n",
        "        \"timestamp\": f\"{int(frame_idx // fps // 3600):02}:{int((frame_idx // fps % 3600) // 60):02}:{int(frame_idx // fps % 60):02}\",\n",
        "        \"person_count\": len(frame_person_ids),\n",
        "        \"car_count\": len(frame_car_ids),\n",
        "        \"motorcycle_count\": len(frame_bike_ids),\n",
        "        \"autorickshaw_count\": len(frame_auto_ids),\n",
        "        \"vehicle_count\": vehicle_count,\n",
        "        \"avg_speed\": round(avg_speed, 2),\n",
        "        \"traffic_condition\": traffic_condition\n",
        "    })\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Save full-frame CSV\n",
        "df = pd.DataFrame(output_data)\n",
        "df.to_csv(\"combined_traffic_counts.csv\", index=False)\n",
        "print(\"âœ… Full-frame CSV saved as combined_traffic_counts.csv\")\n",
        "print(\"âœ… Video saved as output2_annotated.avi\")\n",
        "\n",
        "# Summary every 10 seconds\n",
        "interval_sec = 10\n",
        "interval_frames = int(fps * interval_sec)\n",
        "summarized_data = []\n",
        "\n",
        "for i in range(0, len(output_data), interval_frames):\n",
        "    chunk = output_data[i:i + interval_frames]\n",
        "    if not chunk:\n",
        "        continue\n",
        "    summarized_data.append({\n",
        "        \"interval_start\": chunk[0]['timestamp'],\n",
        "        \"interval_end\": chunk[-1]['timestamp'],\n",
        "        \"total_persons\": chunk[-1]['person_count'],\n",
        "        \"total_cars\": chunk[-1]['car_count'],\n",
        "        \"total_motorcycles\": chunk[-1]['motorcycle_count'],\n",
        "        \"total_autorickshaws\": chunk[-1]['autorickshaw_count'],\n",
        "        \"total_vehicles\": chunk[-1]['vehicle_count'],\n",
        "        \"avg_speed\": round(np.mean([row['avg_speed'] for row in chunk]), 2)\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summarized_data)\n",
        "df_summary.to_csv(f\"summarized_traffic_every_{interval_sec}s.csv\", index=False)\n",
        "print(f\"âœ… Summary CSV saved as summarized_traffic_every_{interval_sec}s.csv\")\n"
      ],
      "metadata": {
        "id": "HAgpLMxszvRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model with Train-Test Split + Real Data Prediction\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ==== Step 1: Define full labeled dataset ====\n",
        "data = pd.DataFrame({\n",
        "    'person_count':    [2, 5, 10, 15, 20, 25, 30, 35, 40],\n",
        "    'vehicle_count':   [3, 6, 9, 12, 15, 18, 22, 28, 35],\n",
        "    'avg_speed':       [3000, 4000, 5000, 6000, 7000, 9000, 11000, 13000, 15000],\n",
        "    'traffic_condition': ['Low', 'Low', 'Low', 'Moderate', 'Moderate', 'Moderate', 'High', 'High', 'High']\n",
        "})\n",
        "\n",
        "# ==== Step 2: Encode labels ====\n",
        "X = data[['person_count', 'vehicle_count', 'avg_speed']]\n",
        "y = data['traffic_condition']\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# ==== Step 3: Split into train and test ====\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# ==== Step 4: Train model ====\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"âœ… Model trained on training data.\")\n",
        "\n",
        "# ==== Step 5: Evaluate model on validation set ====\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"ðŸŽ¯ Accuracy on validation set: {val_accuracy * 100:.2f}%\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred, target_names=le.classes_))\n",
        "\n",
        "# ==== Step 6: Load real-world CSV to predict ====\n",
        "df = pd.read_csv(\"combined_traffic_counts.csv\")\n",
        "\n",
        "required_columns = ['person_count', 'vehicle_count', 'avg_speed']\n",
        "if not all(col in df.columns for col in required_columns):\n",
        "    raise ValueError(f\"âŒ Missing one or more required columns: {required_columns}\")\n",
        "\n",
        "X_real = df[required_columns]\n",
        "predictions_encoded = model.predict(X_real)\n",
        "predicted_labels = le.inverse_transform(predictions_encoded)\n",
        "\n",
        "# ==== Step 7: Map simplified traffic labels ====\n",
        "label_map = {\n",
        "    'Low': 'Light',\n",
        "    'Moderate': 'Moderate',\n",
        "    'High': 'Heavy'\n",
        "}\n",
        "predicted_readable = [label_map[label] for label in predicted_labels]\n",
        "df['predicted_traffic_condition'] = predicted_readable\n",
        "\n",
        "# ==== Step 8: Save results ====\n",
        "df.to_csv(\"traffic_counts_with_predictions.csv\", index=False)\n",
        "print(\"ðŸ“ Frame-wise predictions saved to 'traffic_counts_with_predictions.csv'\")\n",
        "\n",
        "# ==== Step 9: Final summary ====\n",
        "final_traffic = df['predicted_traffic_condition'].value_counts().idxmax()\n",
        "print(f\"\\nðŸš¦ Final Overall Traffic Condition: {final_traffic} traffic\")\n"
      ],
      "metadata": {
        "id": "hAOsR8Ca8goZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}