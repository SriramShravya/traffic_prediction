{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNibkrQkLWeJM4L0Fkrqdm5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriramShravya/traffic_prediction/blob/main/robofl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5ZHt2_I_nGJ"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics roboflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow #from roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"7vYkKz547Z8GPgQrTSMy\")\n",
        "project = rf.workspace(\"traffic-zi58y\").project(\"my-first-project-dcbu4\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "id": "k8TcYV4SAY-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training yolov8 on roboflow dataset\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8s.pt\")  # Better choice for small dataset\n",
        "\n",
        "model.train(\n",
        "    data=\"/content/My-First-Project-1/data.yaml\",\n",
        "    epochs=150,     # or 200 if needed\n",
        "    imgsz=640,\n",
        "    patience=15     # Stop early if no improvement\n",
        ")\n"
      ],
      "metadata": {
        "id": "j0-OQhxJA8tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/My-First-Project-1/data.yaml\n"
      ],
      "metadata": {
        "id": "K3-apMXzNdwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.val()\n"
      ],
      "metadata": {
        "id": "PnH-wgAQN5SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Load trained YOLOv8 model\n",
        "model = YOLO(\"/content/runs/detect/train/weights/best.pt\")\n",
        "\n",
        "# Load input video\n",
        "input_path = \"/content/video10s.mp4\"\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "# Get video properties\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Define output video writer\n",
        "output_path = \"/content/output.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Frame-by-frame inference\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Predict with model\n",
        "    results = model.predict(source=frame, conf=0.5, imgsz=640, verbose=False)\n",
        "\n",
        "    # Draw results on frame\n",
        "    annotated_frame = results[0].plot()  # Draw boxes and labels\n",
        "\n",
        "    # Write annotated frame to output video\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "# Release everything\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"âœ… Output video saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "CFbdIDidOJQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python pandas numpy torchvision\n"
      ],
      "metadata": {
        "id": "6V6jnz1uKNGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n"
      ],
      "metadata": {
        "id": "9QKuVlzQapmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "# Load models\n",
        "auto_model = YOLO(\"/content/runs/detect/train/weights/best.pt\")  # Auto-rickshaw model\n",
        "base_model = YOLO(\"yolov8s.pt\")  # General detection model\n",
        "\n",
        "# Input video\n",
        "video_path = \"/content/video10s.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Output video setup\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(\"output2_annotated.mp4\", fourcc, fps, (width, height))\n",
        "\n",
        "# Tracking\n",
        "frame_idx = 0\n",
        "output_data = []\n",
        "\n",
        "# Unique ID tracking sets\n",
        "counted_person_ids = set()\n",
        "counted_car_ids = set()\n",
        "counted_bike_ids = set()\n",
        "counted_auto_ids = set()\n",
        "\n",
        "# Constants for class IDs (from COCO)\n",
        "PERSON_ID = 0\n",
        "CAR_ID = 2\n",
        "MOTORCYCLE_ID = 3\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Get predictions from both models\n",
        "    auto_result = auto_model.track(source=frame, conf=0.3, imgsz=640, persist=True, verbose=False)[0]\n",
        "    base_result = base_model.track(source=frame, conf=0.3, imgsz=640, persist=True, verbose=False)[0]\n",
        "\n",
        "    auto_boxes = auto_result.boxes\n",
        "    base_boxes = base_result.boxes\n",
        "\n",
        "    person_boxes = []   # List of (box, id) for persons\n",
        "    vehicle_boxes = []  # List of box tensors for cars, bikes, autos\n",
        "\n",
        "    # Auto detection\n",
        "    auto_tensor = torch.stack([box for box in auto_boxes.xyxy.cpu()]) if auto_boxes else torch.empty((0, 4))\n",
        "    auto_ids = auto_boxes.id.cpu() if auto_boxes and auto_boxes.id is not None else torch.full((0,), -1)\n",
        "\n",
        "    for i, box in enumerate(auto_boxes.xyxy.cpu() if auto_boxes else []):\n",
        "        x1, y1, x2, y2 = map(int, box.tolist())\n",
        "        track_id = int(auto_ids[i]) if i < len(auto_ids) else -1\n",
        "        if track_id not in counted_auto_ids:\n",
        "            counted_auto_ids.add(track_id)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
        "        cv2.putText(frame, f\"Auto ID:{track_id}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
        "        vehicle_boxes.append(box.unsqueeze(0))  # Include in vehicle tensor\n",
        "\n",
        "    if base_boxes is not None:\n",
        "        base_cls = base_boxes.cls.cpu()\n",
        "        base_xyxy = base_boxes.xyxy.cpu()\n",
        "        base_ids = base_boxes.id.cpu() if base_boxes.id is not None else torch.full((len(base_cls),), -1)\n",
        "\n",
        "        for i, c in enumerate(base_cls):\n",
        "            box = base_xyxy[i].unsqueeze(0)\n",
        "            x1, y1, x2, y2 = map(int, box.squeeze(0).tolist())\n",
        "            track_id = int(base_ids[i])\n",
        "\n",
        "            # Skip if overlapping with auto\n",
        "            if auto_tensor.shape[0] > 0 and (box_iou(box, auto_tensor) > 0.5).any():\n",
        "                continue\n",
        "\n",
        "            label = \"\"\n",
        "            is_vehicle = False\n",
        "\n",
        "            if int(c.item()) == PERSON_ID:\n",
        "                person_boxes.append((box, track_id))  # Save person box + id\n",
        "                continue  # Draw later if not on vehicle\n",
        "            elif int(c.item()) == CAR_ID:\n",
        "                label = \"Car\"\n",
        "                if track_id not in counted_car_ids:\n",
        "                    counted_car_ids.add(track_id)\n",
        "                is_vehicle = True\n",
        "            elif int(c.item()) == MOTORCYCLE_ID:\n",
        "                label = \"Motorcycle\"\n",
        "                if track_id not in counted_bike_ids:\n",
        "                    counted_bike_ids.add(track_id)\n",
        "                is_vehicle = True\n",
        "\n",
        "            if label:\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, f\"{label} ID:{track_id}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "            if is_vehicle:\n",
        "                vehicle_boxes.append(box)\n",
        "\n",
        "    # Filter and count only walking persons\n",
        "    if person_boxes:\n",
        "        person_tensor = torch.cat([b for b, _ in person_boxes], dim=0)\n",
        "        person_ids = [pid for _, pid in person_boxes]\n",
        "        vehicle_tensor = torch.cat(vehicle_boxes, dim=0) if vehicle_boxes else torch.empty((0, 4))\n",
        "\n",
        "        for i in range(person_tensor.shape[0]):\n",
        "            person_box = person_tensor[i].unsqueeze(0)\n",
        "            track_id = person_ids[i]\n",
        "\n",
        "            is_on_vehicle = False\n",
        "            if vehicle_tensor.shape[0] > 0:\n",
        "                ious = box_iou(person_box, vehicle_tensor)\n",
        "                max_iou = ious.max().item()\n",
        "\n",
        "                if max_iou > 0.3:\n",
        "                    is_on_vehicle = True\n",
        "                else:\n",
        "                    # Vertical overlap check (helps in cases where person is inside bounding box of vehicle)\n",
        "                    px1, py1, px2, py2 = person_box.squeeze().tolist()\n",
        "                    for vbox in vehicle_tensor:\n",
        "                        vx1, vy1, vx2, vy2 = vbox.tolist()\n",
        "                        if px1 > vx1 and px2 < vx2 and py2 < vy2 and py1 > vy1:\n",
        "                            is_on_vehicle = True\n",
        "                            break\n",
        "\n",
        "            if is_on_vehicle:\n",
        "                continue  # Skip persons on vehicles\n",
        "\n",
        "            if track_id not in counted_person_ids:\n",
        "                counted_person_ids.add(track_id)\n",
        "\n",
        "            x1, y1, x2, y2 = map(int, person_box.squeeze().tolist())\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "            cv2.putText(frame, f\"Person ID:{track_id}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "    # Overlay live stats\n",
        "    stats = (\n",
        "        f\"Persons: {len(counted_person_ids)}  \"\n",
        "        f\"Cars: {len(counted_car_ids)}  \"\n",
        "        f\"Bikes: {len(counted_bike_ids)}  \"\n",
        "        f\"Autos: {len(counted_auto_ids)}\"\n",
        "    )\n",
        "    cv2.putText(frame, stats, (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "    # Save data per frame\n",
        "    output_data.append({\n",
        "        \"frame\": frame_idx,\n",
        "        \"timestamp\": f\"{int(frame_idx // fps // 3600):02}:{int((frame_idx // fps % 3600) // 60):02}:{int(frame_idx // fps % 60):02}\",\n",
        "        \"person_count\": len(counted_person_ids),\n",
        "        \"car_count\": len(counted_car_ids),\n",
        "        \"motorcycle_count\": len(counted_bike_ids),\n",
        "        \"autorickshaw_count\": len(counted_auto_ids)\n",
        "    })\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Save full-frame CSV\n",
        "df = pd.DataFrame(output_data)\n",
        "df.to_csv(\"combined_traffic_counts.csv\", index=False)\n",
        "print(\"âœ… Full-frame CSV saved as combined_traffic_counts.csv\")\n",
        "print(\"âœ… Video saved as output2_annotated.mp4\")\n",
        "\n",
        "# 10-second summary\n",
        "interval_sec = 10\n",
        "interval_frames = int(fps * interval_sec)\n",
        "summarized_data = []\n",
        "\n",
        "for i in range(0, len(output_data), interval_frames):\n",
        "    chunk = output_data[i:i + interval_frames]\n",
        "    if not chunk:\n",
        "        continue\n",
        "\n",
        "    start_time = chunk[0]['timestamp']\n",
        "    end_time = chunk[-1]['timestamp']\n",
        "    person_total = chunk[-1]['person_count']\n",
        "    car_total = chunk[-1]['car_count']\n",
        "    bike_total = chunk[-1]['motorcycle_count']\n",
        "    auto_total = chunk[-1]['autorickshaw_count']\n",
        "\n",
        "    summarized_data.append({\n",
        "        \"interval_start\": start_time,\n",
        "        \"interval_end\": end_time,\n",
        "        \"total_persons\": person_total,\n",
        "        \"total_cars\": car_total,\n",
        "        \"total_motorcycles\": bike_total,\n",
        "        \"total_autorickshaws\": auto_total\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summarized_data)\n",
        "df_summary.to_csv(f\"summarized_traffic_every_{interval_sec}s.csv\", index=False)\n",
        "print(f\"âœ… Summary CSV saved as summarized_traffic_every_{interval_sec}s.csv\")\n"
      ],
      "metadata": {
        "id": "HAgpLMxszvRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}